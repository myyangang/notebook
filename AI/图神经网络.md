# 图神经网络

参考书籍：

- 深入浅出图神经网络(GNN原理解析) [豆瓣](https://book.douban.com/subject/34927262/)

# §1 基础概念

## §1.1 图

我们在离散数学中学过，图（Graph）$G$由顶点(Vertex)$V=\{v_1,v_2,...\}$和边(Edge)$E=\{e_1,e_2,...\}$构成。边分为有向边$\langle v_i,v_j \rangle$和无向边$(v_i,v_j)$，记为$e_{ij}$

# §2 图神经网络

表征学习包含图表征学习。图表征学习使用到的图嵌入方法包含图神经网络。

## §2.1 表征学习

我们知道，数据集有可能缺少重要信息、存在冗余信息，甚至包含不正确的信息，从而干扰算法的性能。为了解决这一问题，特征工程应运而生，通过数据预处理和数据转换，从数据中提取机器学习所需的信息。然而，特征工程存在以下两个缺点：

- 依赖领域专家的密集劳动，需要领域专家与模型开发者密切配合
- 受限于领域专家的能力，导致数据不完整、分布存在偏见，限制了提取特征的数量
- 在开放的领域中，无法判定提取什么特征会产生最好的效果

为解决以上问题，表征学习应运而生。表征学习的目标是从数据中提取足够且尽可能少的信息。传统的表征学习负责学习数据转换，例如主成分分析(PCA, 1987)、高斯马尔科夫随机场(GMRF, 2005)、局部保持投影(LPP, 20044)。

表征学习可以分为以下几类：需要利用先验知识的监督学习、可以训练深度网络的无监督学习、增强泛化能力的迁移学习，以及强化学习、小样本学习、解耦表征学习。

| 领域名称\表征学习类别 | 监督学习                                                     | 无监督学习                                                   | 迁移学习 | 强化学习                           |
| --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------- | ---------------------------------- |
| 图像处理              | 卷积神经网络(CNN)<br />深度信念网络(DBN)<br />AlexNet、VGG、GoogLeNet、ResNet、DenseNet | 代理任务：<br />灰度图像着色(2016)<br />图像修复(2016)       |          | 图像描述(2018)<br />图像编辑(2020) |
| 语音识别              | 受限玻尔兹曼机(RBM, 2011)<br />深度信念网络(2016)<br />MAVIS(2012)<br />CNN<br />LSTM/GRU | 变分自编码器(VAE)<br />降噪自编码器(DAE)<br />生成对抗网络(GAN, 2017)<br />对抗性自编码器(AAE, 2017) | MTL      |                                    |
| 自然语言处理          | CNN<br />RNN                                                 | word2vec(2013)<br />GloVe(2014)<br />BERT(2019)<br />循环自编码器<br />SENNA(2011) |          |                                    |
| 网络分析              | 网络表征学习(NRL)                                            |                                                              |          |                                    |

## §2.2 图表征学习

图表征学习面临着以下问题：

- 高计算复杂性：例如为了计算两点之间的距离，我们必须遍历很多路径。
- 低可并行性：每个计算节点都必须得到节点之间的关系$E$，提高了通信成本，限制于通信瓶颈
- 机器学习方法不适用：机器学习假设各神经元相互独立，而图中的节点相互依赖

图表征学习分为三类：传统图嵌入方法、现代图嵌入方法、图神经网络。

- 传统图嵌入方法针对已有数据集构成的图，其目标是进行图的重建(也就是降维)。Tenenbaum等人于2000年从KNN构建的邻接图中提取出图距离矩阵，然后经过多维尺度变换(MDS)得到特征向量。Roweis等人于2000年提出了局部线性嵌入(LLE)。Belkin等人于2002年提出了拉普拉斯特征映射(LE)，通过热核选择图中两个节点的权重，经过拉普拉斯矩阵正则化得到特征向量。Berline等人于2003年提出局部保持投影(LPP)，作为非线性LE的线性近似算法。
- 现代图嵌入方法针对自然形成的网络，其目标是支持图推理。Perozzi等人于2014年提出了DeepWalk。node2vec定义了灵活的节点图邻域，通过二阶随机行走策略对邻域节点进行抽样。Tang等人与2015年提出了LINE，用于保留一阶和二阶的接近度（两个节点之间成对节点的接近程度）。Wang等人于2016年提出了SDNE，同样用于保留一阶和二阶的接近度。Wang等人于2017年提出了非负矩阵因子化模型(M-NMF)，用于同时保留一阶和二阶的接近度与中观群落结构。



TODO

回想传统的神经网络，我们从样本中提取一个一维特征向量，送到大小不一的神经网络层。这些层可能是全连接层、池化层等等，因此各层之间的尺寸有可能不同。输入向量中的任意一个标量，都会在前向传播的过程中扩散开来。一个神经元收到多个输入值时，采取的策略是相加。

图神经网络有所不同。第一：各层的尺寸、拓扑结构完全相同。第二：输入的不再是一维特征向量，而是包含顶点信息、边信息的图。第三：前向传播不再使用全连接层，也就是说前一层的神经元不会影响到后一层的所有神经元，只会影响在图的拓扑结构中相邻的神经元。第四：一个神经元收到多个输入值时，可以取最大值、平均值或直接相加。

基于此，我们可以用数学符号描述图神经网络：

| 符号                                               | 符号含义                                                     |
| -------------------------------------------------- | ------------------------------------------------------------ |
| $\mathscr{G}=\left(\mathscr{V},\mathscr{E}\right)$ | 图由顶点集合与边集合构成。                                   |
| $\mathscr{V}=\{v_1,v_2,...,v_N\}$                  | 顶点总数为$N$。                                              |
| $\mathbf{X}\in\R^{N\times C}$                      | 输入信息中顶点携带的向量信息构成的矩阵，向量维度为$C$。      |
| $\mathbf{h}_i^{k}\in\R^{C}$                        | 第$k$层中第$i$个顶点携带的向量信息，维度为$C$。              |
| $\mathbf{H}^{k}\in\R^{N\times F},1\le k\le K$      | 第$k$层中顶点携带的向量信息构成的矩阵，总层数为$K$。         |
| $\mathbf{A}\in\R^{N\times N}$                      | 邻接矩阵。由于图神经网络各层拓扑结构完全相同，所以只用一个矩阵就能表示所有层的邻接矩阵。数据类型为实数，表示连接程度，类似于权重，其中$0$表示完全不相邻。 |
| $N(v_i)=\{v_j|\mathbf{A}_{ij}>0\}$                 | 在邻接矩阵中，与顶点$v_i$相邻的其它顶点构成的集合。          |
| $\mathbf{\hat{y}}_i\in\R^{L},1\le i\le N$          | 图神经网络的分类结果向量，预期的分类类别有$L$种。            |

给定初始条件：
$$
\mathbf{H}^0=\mathbf{X}
$$
进行图神经网络的前向传播：
$$
\begin{cases}
	\mathbf{H}^0=\mathbf{X} \\
	\mathbf{h}_{i}^{k}=\text{sigmoid}(\text{Combine}(\mathbf{h}_{i}^{k-1},N(v_i))) & ,1\le i\le N,1\le k\le K
\end{cases}
$$
最后接上全连接层，用于输出每个节点的类别预测向量：
$$
\mathbf{\hat{y}}_i = \text{softmax}(\mathbf{W}(\mathbf{h}^{K}_i)^T)\in\R^{L\times 1}
\\
\mathbf{\hat{Y}} = \text{softmax}(\mathbf{W}(\mathbf{H}^K)^T)\in\R^{L\times N}
$$
问题转换为最优化损失函数：
$$
\underset{\mathbf{A}}{\text{argmin}}\ \mathcal{L} 
= \underset{\mathbf{A}}{\text{argmin}}\ \sum_{i=1}^{N}||\mathbf{\hat{y}}_i-\mathbf{y}_i||_2^2
= \underset{\mathbf{A}}{\text{argmin}}\ ||\mathbf{\hat{Y}}-\mathbf{Y}||_2^2
$$

# §3 编程实现





